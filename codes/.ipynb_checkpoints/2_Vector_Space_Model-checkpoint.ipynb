{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2回 ベクトル空間モデル\n",
    "\n",
    "この演習ページでは，ベクトル空間モデルに基づく情報検索モデルについて説明します．具体的には，文書から特徴ベクトルへの変換方法，TF-IDFの計算方法，コサイン類似度による文書ランキングについて，その実装例を説明します．第2回演習の最終目的は，ある与えられた文書コーパスに対して，TF-IDFで重み付けされた特徴ベクトルによる文書ランキングが実装できるようになることです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリ\n",
    "この回の演習では，以下のライブラリを使用します．　\n",
    "- [numpy, scipy](http://www.numpy.org/)\n",
    "  + Pythonで科学技術計算を行うための基礎的なライブラリ．\n",
    "- [gensim](https://radimrehurek.com/gensim/index.html)\n",
    "  + トピックモデリング（LDA）やword2vecなどを手軽に利用するためのPythonライブラリ．\n",
    "- [nltk (natural language toolkit)](http://www.nltk.org/)\n",
    "  + 自然言語処理に関するpythonライブラリです．この演習ではストップワードのために用います．ほかにも，単語のステミングやトークナイズなどの機能をはじめ，品詞推定，依存関係分析など自然言語処理のあらゆるメソッドが用意されています．\n",
    "- [pandas](http://pandas.pydata.org/)\n",
    "  + pythonでデータ分析をするためのフレームワークです．この演習ではデータをプロットするために用いています．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第2回目の演習の内容\n",
    "``h29iro/data/`` に `sample.corpus` というファイルを置いています． このファイルには改行区切りで3件の短い文書が保存されています．この演習では，このファイルに対してTF-IDFで重み付けされた特徴ベクトルを作成し，コサイン類似度によるランキングを行います．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 文書の読み込みとトークナイズ\n",
    "まずは，`sample.corpus`を読み込み，各文書のBoW表現を抽出します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.3f'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 小数点3ケタまで表示\n",
    "%precision 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I live in kyoto and kyoto is a beautiful city',\n",
       " 'kyoto was the captial of japan and is in kansai and kansai is in japan',\n",
       " 'kyoto is in kansai and kyoto is historical city']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/sample.corpus\", \"r\") as f:  #sample.corpusの読み込み\n",
    "    text = f.read().strip().split(\"\\n\") #sample.corpusのテキストデータを取得し，それを改行で分割\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "３件の文書があることが分かりますね．次に，文章をトークン（単語）に分割します．今回は簡単のため単純にスペース区切りによって単語に分割します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1= ['i', 'live', 'in', 'kyoto', 'and', 'kyoto', 'is', 'a', 'beautiful', 'city']\n",
      "d2= ['kyoto', 'was', 'the', 'captial', 'of', 'japan', 'and', 'is', 'in', 'kansai', 'and', 'kansai', 'is', 'in', 'japan']\n",
      "d3= ['kyoto', 'is', 'in', 'kansai', 'and', 'kyoto', 'is', 'historical', 'city']\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = [d.lower().split() for d in text] #文章を小文字に変換して単語に分割する\n",
    "print(\"d1=\" , raw_corpus[0])\n",
    "print(\"d2=\" , raw_corpus[1])\n",
    "print(\"d3=\" , raw_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文が単語の集合に変換されました．しかし，この単語集合には \"i\" や \"of\" などのストップワードが含まれています．そこで，ストップワードを除去してみましょう．\n",
    "\n",
    "ストップワードのリストはネットで探せば様々な種類が見つかります．ここでは，nltkのstopwordsモジュールを利用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1= ['live', 'kyoto', 'kyoto', 'beautiful', 'city']\n",
      "d2= ['kyoto', 'captial', 'japan', 'kansai', 'kansai', 'japan']\n",
      "d3= ['kyoto', 'kansai', 'kyoto', 'historical', 'city']\n"
     ]
    }
   ],
   "source": [
    "# stopwords.words(\"english\")に含まれていない単語のみ抽出\n",
    "corpus = [list(filter(lambda word: word not in stopwords.words(\"english\"), x)) for x in raw_corpus] \n",
    "print(\"d1=\" , corpus[0])\n",
    "print(\"d2=\" , corpus[1])\n",
    "print(\"d3=\" , corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特徴ベクトルの生成\n",
    "次に文書の特徴ベクトルを生成します．ここからの流れは，以下の通りになります．\n",
    "\n",
    "1. 文書集合（corpus）から 単語->単語ID の辞書 (dictionary) を作成する．\n",
    "2. 作成された辞書を基に，文書を (単語ID，出現回数）の集合 (id_corpus) として表現する．\n",
    "3. id_corpusからTfidfModelを用いて，TF-IDFで重み付けされた特徴ベクトルを作成する．\n",
    "\n",
    "まずは，文書集合（コーパス）から単語->単語ID の辞書 (dictionary) を作成します．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beautiful': 1,\n",
       " 'captial': 6,\n",
       " 'city': 2,\n",
       " 'historical': 7,\n",
       " 'japan': 5,\n",
       " 'kansai': 4,\n",
       " 'kyoto': 0,\n",
       " 'live': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(corpus) #コーパスを与えて，単語->IDの辞書を作成する\n",
    "dictionary.token2id #作成された辞書の中身"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このdictionaryを用いて，文書の単語をID化します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 1), (3, 1)],\n",
       " [(0, 1), (4, 2), (5, 2), (6, 1)],\n",
       " [(0, 2), (2, 1), (4, 1), (7, 1)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_corpus = [dictionary.doc2bow(document) for document in corpus]\n",
    "id_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成されたid_corpusは，たとえば，1件目の文書は"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (1, 1), (2, 1), (3, 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "という内容になっています．たとえば，(0,2)というデータは\n",
    "```\n",
    "単語ID0の単語が２回出現\n",
    "```\n",
    "という内容を表しています． つまり，単語の出現頻度(term frequency)のみで文書を特徴ベクトル化したことになります．なお，これをnumpyのベクトルとして抽出したければ，corpus2denseメソッドを用います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1= [ 2.  1.  1.  1.  0.  0.  0.  0.]\n",
      "d2= [ 1.  0.  0.  0.  2.  2.  1.  0.]\n",
      "d3= [ 2.  0.  1.  0.  1.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "tf_vectors = gensim.matutils.corpus2dense(id_corpus, len(dictionary)).T\n",
    "print(\"d1=\", tf_vectors[0])\n",
    "print(\"d2=\", tf_vectors[1])\n",
    "print(\"d3=\", tf_vectors[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回用意したコーパスは語彙数が8しかありませんが，実際のケースでは，この特徴ベクトルは非常に疎になることが容易に想像つくと思います．\n",
    "\n",
    "さて，id_corpusからTFIDFで重み付けされた特徴ベクトルを得るには， models.TfidfModel メソッドを用います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_model = gensim.models.TfidfModel(id_corpus, normalize=False) #normalize=Trueにすると，文書長によってtfを正規化する\n",
    "tfidf_corpus = tfidf_model[id_corpus] #id_corpusをtfidfで重み付けされたものに変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでTF-IDFで重み付けされた特徴ベクトルが得られました．たとえば，1件目の文書$d_1$に対する特徴ベクトル${\\mathbf d}_1$の中身を見てみます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1.585), (2, 0.585), (3, 1.585)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDFの値は，(単語ID，重み） として得られています．単語IDを実際の単語に変換するにはdictionaryを通します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beautiful', 1.585), ('city', 0.585), ('live', 1.585)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(dictionary[x[0]], x[1]) for x in tfidf_corpus[0]]#dictionary[token_id]でアクセスすると実際の単語が返ってくる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同様に2件目の文書$d_2$についても見てみます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kansai', 1.170), ('japan', 3.170), ('captial', 1.585)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = [(dictionary[x[0]], x[1]) for x in tfidf_corpus[1]]\n",
    "doc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たとえば， 文書$d_{2}$における`japan`のTFIDF値が本当に正しいのか検証してみましょう．\n",
    "\n",
    "$tfidf_{d_2, japan} = tf_{d_2, japan} \\log \\frac{N}{df_{japan}}$ ,\n",
    "\n",
    "いま， $tf_{d_2, japan} = 2$, $N = 3$, $df_{japan}$ = 1 ですので，\n",
    "\n",
    "$tfidf_{d_2, japan} = 2 \\log 3 = 3.170$\n",
    "となり，gensimで得られた結果と一致していることが分かります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.170"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "2*math.log2(3) #2log3の計算方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. コサイン類似度\n",
    "それでは，コサイン類似度による文書ランキングを行ってみましょう．\n",
    "\n",
    "クエリと文書の類似度を測る前に，まずは文書同士のコサイン類似度を計算してみます． コサイン類似度の計算はgensimでも良いのですが，ここでは，いったんnumpyのベクトルを取得して，そのベクトルに対してコサイン類似度を計算してみます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1= [ 0.     1.585  0.585  1.585  0.     0.     0.     0.   ]\n",
      "d2= [ 0.     0.     0.     0.     1.17   3.17   1.585  0.   ]\n",
      "d3= [ 0.     0.     0.585  0.     0.585  0.     0.     1.585]\n"
     ]
    }
   ],
   "source": [
    "# 各文書のtfidfベクトルを取得\n",
    "tfidf_vectors = gensim.matutils.corpus2dense(tfidf_corpus, len(dictionary)).T\n",
    "print (\"d1=\", tfidf_vectors[0])\n",
    "print (\"d2=\", tfidf_vectors[1])\n",
    "print (\"d3=\", tfidf_vectors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# コサイン類似度を計算する関数を用意\n",
    "from scipy.spatial.distance import cosine\n",
    "def cosine_sim(v1, v2):\n",
    "    #scipyのcosineは類似度ではなく距離関数のため， 1-コサイン距離　とすることで，コサイン類似度に変換する\n",
    "    return 1.0 - cosine(v1, v2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim(d1, d2)= 0.0\n",
      "sim(d2, d3)= 0.102562095083\n",
      "sim(d1, d3)= 0.082618937993\n"
     ]
    }
   ],
   "source": [
    "# 各文書間のコサイン類似度を計算してみる\n",
    "print (\"sim(d1, d2)=\", cosine_sim(tfidf_vectors[0], tfidf_vectors[1]))\n",
    "print (\"sim(d2, d3)=\", cosine_sim(tfidf_vectors[1], tfidf_vectors[2]))\n",
    "print (\"sim(d1, d3)=\", cosine_sim(tfidf_vectors[0], tfidf_vectors[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは，クエリを特徴ベクトルに変換し，クエリと文書のコサイン類似度を求めていきましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q= [ 0.     0.     0.     0.     0.585  1.585  0.     0.   ]\n"
     ]
    }
   ],
   "source": [
    "q = {\"kansai\", \"japan\"}\n",
    "tfidf_q = tfidf_model[dictionary.doc2bow(q)]  #クエリをtfidfベクトルに変換\n",
    "query_vector = gensim.matutils.corpus2dense([tfidf_q], len(dictionary)).T[0] #numpyのベクトルに変換\n",
    "print (\"q=\", query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('kansai', 0.5849625007211562), ('japan', 1.5849625007211563)]\n"
     ]
    }
   ],
   "source": [
    "print([(dictionary[x[0]], x[1]) for x in tfidf_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim(q, d1) =  0.0\n",
      "sim(q, d2) =  0.905346644389\n",
      "sim(q, d3) =  0.113284893168\n"
     ]
    }
   ],
   "source": [
    "print (\"sim(q, d1) = \", cosine_sim(query_vector, tfidf_vectors[0]))\n",
    "print (\"sim(q, d2) = \", cosine_sim(query_vector, tfidf_vectors[1]))\n",
    "print (\"sim(q, d3) = \", cosine_sim(query_vector, tfidf_vectors[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この結果から，q={\"kansai\", \"japan\"} というクエリに対しては，$d_2,d_3, d_1$の順でランク付けされることが分かります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ベクトル空間の可視化\n",
    "\n",
    "最後に，得られた特徴ベクトルを可視化してみましょう．特徴ベクトルそのものは多次元（今回の場合は8次元）ですが，これを次元削減の手法を使って，2次元空間に射影してみます．今回は，`LSI`（Latent Semantic Indexing)という手法を用いて，特徴ベクトルを2次元空間に落とし込みます．LSIについては，講義で触れるかもしれません（講義の進み方次第）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1= [-0.009  2.302]\n",
      "d2= [-3.73  -0.028]\n",
      "d3= [-0.237  0.346]\n",
      "q= [-1.53  -0.007]\n"
     ]
    }
   ],
   "source": [
    "# LSIにより特徴ベクトルを2次元に落とし込む\n",
    "lsi = gensim.models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=2)\n",
    "lsi_corpus = lsi[tfidf_corpus]\n",
    "lsi_vectors = gensim.matutils.corpus2dense(lsi_corpus, 2).T\n",
    "print(\"d1=\", lsi_vectors[0])\n",
    "print(\"d2=\", lsi_vectors[1])\n",
    "print(\"d3=\", lsi_vectors[2])\n",
    "query_lsi_corpus = lsi[[tfidf_q]] \n",
    "query_lsi_vector = gensim.matutils.corpus2dense(query_lsi_corpus, 2).T[0]\n",
    "print (\"q=\", query_lsi_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z1</th>\n",
       "      <th>z2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d1</th>\n",
       "      <td>-0.009431</td>\n",
       "      <td>2.301599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d2</th>\n",
       "      <td>-3.730471</td>\n",
       "      <td>-0.027818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d3</th>\n",
       "      <td>-0.237208</td>\n",
       "      <td>0.345971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>-1.529893</td>\n",
       "      <td>-0.007460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          z1        z2\n",
       "d1 -0.009431  2.301599\n",
       "d2 -3.730471 -0.027818\n",
       "d3 -0.237208  0.345971\n",
       "q  -1.529893 -0.007460"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 散布図にプロットするため，DataFrameに変換\n",
    "axis_names = [\"z1\", \"z2\"]\n",
    "doc_names = [\"d1\", \"d2\", \"d3\", \"q\"]\n",
    "df = pd.DataFrame(np.r_[lsi_vectors, [query_lsi_vector]], \n",
    "                  columns=axis_names, index=doc_names) # np.r_ は行列同士の連結\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEPCAYAAABsj5JaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRQVERERSUQEREZFU/j98S/GqEjgHugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12693a080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 散布図をプロット\n",
    "fig, ax = plt.subplots()\n",
    "df.plot.scatter(x=\"z1\", y=\"z2\", ax=ax)\n",
    "ax.axvline(x=0, lw=2, color='red') #x軸とy軸に線を引く\n",
    "ax.axhline(y=0, lw=2, color='red') \n",
    "ax.grid(True)\n",
    "for k, v in df.iterrows():\n",
    "    ax.annotate(k, xy=(v[0]+0.05,v[1]+0.05),size=15) #データ点にラベル名を付与"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この図を見てみると，やはりクエリ$q$と文書$d_2$はほぼ同じ方向（つまり，コサイン類似度が１に近い）であることがわかり， $q$と$d_1$の角度はほぼ直角（つまりコサイン類似度が0）であることがわかります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習課題\n",
    "\n",
    "工事中"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
