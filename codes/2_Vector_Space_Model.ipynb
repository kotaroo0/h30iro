{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2回 ベクトル空間モデル\n",
    "\n",
    "この演習ページでは，ベクトル空間モデルに基づく情報検索モデルについて説明します．具体的には，文書から特徴ベクトルへの変換方法，TF-IDFの計算方法，コサイン類似度による文書ランキングについて，その実装例を説明します．第2回演習の最終目的は，ある与えられた文書コーパスに対して，TF-IDFで重み付けされた特徴ベクトルによる文書ランキングが実装できるようになることです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリ\n",
    "この回の演習では，以下のライブラリを使用します．　\n",
    "- [numpy, scipy](http://www.numpy.org/)\n",
    "  + Pythonで科学技術計算を行うための基礎的なライブラリ．\n",
    "- [gensim](https://radimrehurek.com/gensim/index.html)\n",
    "  + トピックモデリング（LDA）やword2vecなどを手軽に利用するためのPythonライブラリ．\n",
    "- [nltk (natural language toolkit)](http://www.nltk.org/)\n",
    "  + 自然言語処理に関するpythonライブラリです．この演習ではストップワードのために用います．ほかにも，単語のステミングやトークナイズなどの機能をはじめ，品詞推定，依存関係分析など自然言語処理のあらゆるメソッドが用意されています．\n",
    "- [pandas](http://pandas.pydata.org/)\n",
    "  + pythonでデータ分析をするためのフレームワークです．この演習ではデータをプロットするために用いています．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "\n",
    "本演習では，gensimとnltkを利用します．\n",
    "まず，ターミナルを開き以下のコマンドでgensimをインストールしておいてください．\n",
    "\n",
    "```\n",
    "conda install gensim\n",
    "```\n",
    "\n",
    "次に，pythonを起動し，以下のコマンドから，nltkのcorpusからstopwordsをインストールしておいてください．\n",
    "\n",
    "```\n",
    "python \n",
    ">>> import nltk\n",
    ">>> nltk.download()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第2回目の演習の内容\n",
    "``h29iro/data/`` に `sample.corpus` というファイルを置いています． このファイルには改行区切りで3件の短い文書が保存されています．この演習では，このファイルに対してTF-IDFで重み付けされた特徴ベクトルを作成し，コサイン類似度によるランキングを行います．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 文書の読み込みとトークナイズ\n",
    "まずは，`sample.corpus`を読み込み，各文書のBoW表現を抽出します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download()\n",
    "import pandas as pd\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'%.3f'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 小数点3ケタまで表示\n",
    "%precision 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I live in kyoto and kyoto is a beautiful city',\n",
       " 'kyoto was the captial of japan and is in kansai and kansai is in japan',\n",
       " 'kyoto is in kansai and kyoto is historical city']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/sample.corpus\", \"r\") as f:  #sample.corpusの読み込み\n",
    "    text = f.read().strip().split(\"\\n\") #sample.corpusのテキストデータを取得し，それを改行で分割\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "３件の文書があることが分かりますね．次に，文章をトークン（単語）に分割します．今回は簡単のため単純にスペース区切りによって単語に分割します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d1=', ['i', 'live', 'in', 'kyoto', 'and', 'kyoto', 'is', 'a', 'beautiful', 'city'])\n",
      "('d2=', ['kyoto', 'was', 'the', 'captial', 'of', 'japan', 'and', 'is', 'in', 'kansai', 'and', 'kansai', 'is', 'in', 'japan'])\n",
      "('d3=', ['kyoto', 'is', 'in', 'kansai', 'and', 'kyoto', 'is', 'historical', 'city'])\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = [d.lower().split() for d in text] #文章を小文字に変換して単語に分割する\n",
    "print(\"d1=\" , raw_corpus[0])\n",
    "print(\"d2=\" , raw_corpus[1])\n",
    "print(\"d3=\" , raw_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文が単語の集合に変換されました．しかし，この単語集合には \"i\" や \"of\" などのストップワードが含まれています．そこで，ストップワードを除去してみましょう．\n",
    "\n",
    "ストップワードのリストはネットで探せば様々な種類が見つかります．ここでは，nltkのstopwordsモジュールを利用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d1=', ['live', 'kyoto', 'kyoto', 'beautiful', 'city'])\n",
      "('d2=', ['kyoto', 'captial', 'japan', 'kansai', 'kansai', 'japan'])\n",
      "('d3=', ['kyoto', 'kansai', 'kyoto', 'historical', 'city'])\n"
     ]
    }
   ],
   "source": [
    "# stopwords.words(\"english\")に含まれていない単語のみ抽出\n",
    "corpus = [list(filter(lambda word: word not in stopwords.words(\"english\"), x)) for x in raw_corpus] \n",
    "print(\"d1=\" , corpus[0])\n",
    "print(\"d2=\" , corpus[1])\n",
    "print(\"d3=\" , corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特徴ベクトルの生成\n",
    "次に文書の特徴ベクトルを生成します．ここからの流れは，以下の通りになります．\n",
    "\n",
    "1. 文書集合（corpus）から 単語->単語ID の辞書 (dictionary) を作成する．\n",
    "2. 作成された辞書を基に，文書を (単語ID，出現回数）の集合 (id_corpus) として表現する．\n",
    "3. id_corpusからTfidfModelを用いて，TF-IDFで重み付けされた特徴ベクトルを作成する．\n",
    "\n",
    "まずは，文書集合（コーパス）から単語->単語ID の辞書 (dictionary) を作成します．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'beautiful': 0,\n",
       " u'captial': 4,\n",
       " u'city': 1,\n",
       " u'historical': 7,\n",
       " u'japan': 5,\n",
       " u'kansai': 6,\n",
       " u'kyoto': 2,\n",
       " u'live': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(corpus) #コーパスを与えて，単語->IDの辞書を作成する\n",
    "dictionary.token2id #作成された辞書の中身"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このdictionaryを用いて，文書の単語をID化します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 2), (3, 1)],\n",
       " [(2, 1), (4, 1), (5, 2), (6, 2)],\n",
       " [(1, 1), (2, 2), (6, 1), (7, 1)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_corpus = [dictionary.doc2bow(document) for document in corpus]\n",
    "id_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成されたid_corpusは，たとえば，1件目の文書は"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 2), (3, 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "という内容になっています．たとえば，(0,2)というデータは\n",
    "```\n",
    "単語ID0の単語が２回出現\n",
    "```\n",
    "という内容を表しています． つまり，単語の出現頻度(term frequency)のみで文書を特徴ベクトル化したことになります．なお，これをnumpyのベクトルとして抽出したければ，corpus2denseメソッドを用います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d1=', array([1., 1., 2., 1., 0., 0., 0., 0.], dtype=float32))\n",
      "('d2=', array([0., 0., 1., 0., 1., 2., 2., 0.], dtype=float32))\n",
      "('d3=', array([0., 1., 2., 0., 0., 0., 1., 1.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "tf_vectors = gensim.matutils.corpus2dense(id_corpus, len(dictionary)).T\n",
    "print(\"d1=\", tf_vectors[0])\n",
    "print(\"d2=\", tf_vectors[1])\n",
    "print(\"d3=\", tf_vectors[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回用意したコーパスは語彙数が8しかありませんが，実際のケースでは，この特徴ベクトルは非常に疎になることが容易に想像つくと思います．\n",
    "\n",
    "さて，id_corpusからTFIDFで重み付けされた特徴ベクトルを得るには， models.TfidfModel メソッドを用います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_model = gensim.models.TfidfModel(id_corpus, normalize=False) #normalize=Trueにすると，文書長によってtfを正規化する\n",
    "tfidf_corpus = tfidf_model[id_corpus] #id_corpusをtfidfで重み付けされたものに変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでTF-IDFで重み付けされた特徴ベクトルが得られました．たとえば，1件目の文書$d_1$に対する特徴ベクトル${\\mathbf d}_1$の中身を見てみます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.585), (1, 0.585), (3, 1.585)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDFの値は，(単語ID，重み） として得られています．単語IDを実際の単語に変換するにはdictionaryを通します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'beautiful', 1.585), (u'city', 0.585), (u'live', 1.585)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(dictionary[x[0]], x[1]) for x in tfidf_corpus[0]]#dictionary[token_id]でアクセスすると実際の単語が返ってくる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同様に2件目の文書$d_2$についても見てみます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'captial', 1.585), (u'japan', 3.170), (u'kansai', 1.170)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = [(dictionary[x[0]], x[1]) for x in tfidf_corpus[1]]\n",
    "doc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たとえば， 文書$d_{2}$における`japan`のTFIDF値が本当に正しいのか検証してみましょう．\n",
    "\n",
    "$tfidf_{d_2, japan} = tf_{d_2, japan} \\log \\frac{N}{df_{japan}}$ ,\n",
    "\n",
    "いま， $tf_{d_2, japan} = 2$, $N = 3$, $df_{japan}$ = 1 ですので，\n",
    "\n",
    "$tfidf_{d_2, japan} = 2 \\log 3 = 3.170$\n",
    "となり，gensimで得られた結果と一致していることが分かります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.170"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "2* math.log(3,2) #2log3の計算方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. コサイン類似度\n",
    "それでは，コサイン類似度による文書ランキングを行ってみましょう．\n",
    "\n",
    "クエリと文書の類似度を測る前に，まずは文書同士のコサイン類似度を計算してみます． コサイン類似度の計算はgensimでも良いのですが，ここでは，いったんnumpyのベクトルを取得して，そのベクトルに対してコサイン類似度を計算してみます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d1=', array([1.585, 0.585, 0.   , 1.585, 0.   , 0.   , 0.   , 0.   ],\n",
      "      dtype=float32))\n",
      "('d2=', array([0.   , 0.   , 0.   , 0.   , 1.585, 3.17 , 1.17 , 0.   ],\n",
      "      dtype=float32))\n",
      "('d3=', array([0.   , 0.585, 0.   , 0.   , 0.   , 0.   , 0.585, 1.585],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# 各文書のtfidfベクトルを取得\n",
    "tfidf_vectors = gensim.matutils.corpus2dense(tfidf_corpus, len(dictionary)).T\n",
    "print (\"d1=\", tfidf_vectors[0])\n",
    "print (\"d2=\", tfidf_vectors[1])\n",
    "print (\"d3=\", tfidf_vectors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# コサイン類似度を計算する関数を用意\n",
    "from scipy.spatial.distance import cosine\n",
    "def cosine_sim(v1, v2):\n",
    "    #scipyのcosineは類似度ではなく距離関数のため， 1-コサイン距離　とすることで，コサイン類似度に変換する\n",
    "    return 1.0 - cosine(v1, v2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sim(d1, d2)=', 0.0)\n",
      "('sim(d2, d3)=', 0.10256209224462509)\n",
      "('sim(d1, d3)=', 0.08261892944574356)\n"
     ]
    }
   ],
   "source": [
    "# 各文書間のコサイン類似度を計算してみる\n",
    "print (\"sim(d1, d2)=\", cosine_sim(tfidf_vectors[0], tfidf_vectors[1]))\n",
    "print (\"sim(d2, d3)=\", cosine_sim(tfidf_vectors[1], tfidf_vectors[2]))\n",
    "print (\"sim(d1, d3)=\", cosine_sim(tfidf_vectors[0], tfidf_vectors[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは，クエリを特徴ベクトルに変換し，クエリと文書のコサイン類似度を求めていきましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('q=', array([0.   , 0.   , 0.   , 0.   , 0.   , 1.585, 0.585, 0.   ],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "q = {\"kansai\", \"japan\"}\n",
    "tfidf_q = tfidf_model[dictionary.doc2bow(q)]  #クエリをtfidfベクトルに変換\n",
    "query_vector = gensim.matutils.corpus2dense([tfidf_q], len(dictionary)).T[0] #numpyのベクトルに変換\n",
    "print (\"q=\", query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'japan', 1.5849625007211563), (u'kansai', 0.5849625007211562)]\n"
     ]
    }
   ],
   "source": [
    "print([(dictionary[x[0]], x[1]) for x in tfidf_q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sim(q, d1) = ', 0.0)\n",
      "('sim(q, d2) = ', 0.9053466320037842)\n",
      "('sim(q, d3) = ', 0.1132848858833313)\n"
     ]
    }
   ],
   "source": [
    "print (\"sim(q, d1) = \", cosine_sim(query_vector, tfidf_vectors[0]))\n",
    "print (\"sim(q, d2) = \", cosine_sim(query_vector, tfidf_vectors[1]))\n",
    "print (\"sim(q, d3) = \", cosine_sim(query_vector, tfidf_vectors[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この結果から，q={\"kansai\", \"japan\"} というクエリに対しては，$d_2,d_3, d_1$の順でランク付けされることが分かります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ベクトル空間の可視化\n",
    "\n",
    "最後に，得られた特徴ベクトルを可視化してみましょう．特徴ベクトルそのものは多次元（今回の場合は8次元）ですが，これを次元削減の手法を使って，2次元空間に射影してみます．今回は，`LSI`（Latent Semantic Indexing)という手法を用いて，特徴ベクトルを2次元空間に落とし込みます．LSIについては，講義で触れるかもしれません（講義の進み方次第）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d1=', array([-0.009, -2.302], dtype=float32))\n",
      "('d2=', array([-3.73 ,  0.028], dtype=float32))\n",
      "('d3=', array([-0.237, -0.346], dtype=float32))\n",
      "('q=', array([-1.53 ,  0.007], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# LSIにより特徴ベクトルを2次元に落とし込む\n",
    "lsi = gensim.models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=2)\n",
    "lsi_corpus = lsi[tfidf_corpus]\n",
    "lsi_vectors = gensim.matutils.corpus2dense(lsi_corpus, 2).T\n",
    "print(\"d1=\", lsi_vectors[0])\n",
    "print(\"d2=\", lsi_vectors[1])\n",
    "print(\"d3=\", lsi_vectors[2])\n",
    "query_lsi_corpus = lsi[[tfidf_q]] \n",
    "query_lsi_vector = gensim.matutils.corpus2dense(query_lsi_corpus, 2).T[0]\n",
    "print (\"q=\", query_lsi_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z1</th>\n",
       "      <th>z2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d1</th>\n",
       "      <td>-0.009431</td>\n",
       "      <td>-2.301599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d2</th>\n",
       "      <td>-3.730471</td>\n",
       "      <td>0.027818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d3</th>\n",
       "      <td>-0.237208</td>\n",
       "      <td>-0.345971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>-1.529893</td>\n",
       "      <td>0.007460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          z1        z2\n",
       "d1 -0.009431 -2.301599\n",
       "d2 -3.730471  0.027818\n",
       "d3 -0.237208 -0.345971\n",
       "q  -1.529893  0.007460"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 散布図にプロットするため，DataFrameに変換\n",
    "axis_names = [\"z1\", \"z2\"]\n",
    "doc_names = [\"d1\", \"d2\", \"d3\", \"q\"]\n",
    "df = pd.DataFrame(np.r_[lsi_vectors, [query_lsi_vector]], \n",
    "                  columns=axis_names, index=doc_names) # np.r_ は行列同士の連結\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEMCAYAAADu7jDJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2751f350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 散布図をプロット\n",
    "fig, ax = plt.subplots()\n",
    "df.plot.scatter(x=\"z1\", y=\"z2\", ax=ax)\n",
    "ax.axvline(x=0, lw=2, color='red') #x軸とy軸に線を引く\n",
    "ax.axhline(y=0, lw=2, color='red') \n",
    "ax.grid(True)\n",
    "for k, v in df.iterrows():\n",
    "    ax.annotate(k, xy=(v[0]+0.05,v[1]+0.05),size=15) #データ点にラベル名を付与"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この図を見てみると，やはりクエリ$q$と文書$d_2$はほぼ同じ方向（つまり，コサイン類似度が１に近い）であることがわかり， $q$と$d_1$の角度はほぼ直角（つまりコサイン類似度が0）であることがわかります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習課題その1 ベクトル空間モデル\n",
    "\n",
    "## 必須課題（1） 与えられたコーパスに対する検索の実現\n",
    "\n",
    "以下からコーパスを1つ以上選択し，ベクトル空間モデルに基づいた検索を実現せよ．３種類以上のクエリでの検索結果を示すこと．\n",
    "\n",
    "\n",
    "1. 京都観光に関する83件の文書（h29iro/data/kyoto_results_100.json）\n",
    "2. 各自で用意したコーパス．ただし，100件以上の文書数を含むこと．もっと多くてもよい．\n",
    "3. Wikipedia（[参考: gensim Tutorial](https://radimrehurek.com/gensim/wiki.html) ）※ただし，モデル構築にとんでもない時間がかかるそうなので覚悟すること．\n",
    "\n",
    "\n",
    "- ページに表示する検索結果は各クエリ5-10件程度で良い．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num of docs = ', 83)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'bow': u'\\u5b9a\\u756a \\u7a74\\u5834 \\u304a \\u30b9\\u30b9\\u30e1 \\u4eac\\u90fd \\u89b3\\u5149 ...- NAVER \\u307e\\u3068\\u3081 \\u95a2\\u897f \\u4f4f\\u3093 \\u3044\\u308b \\u4f5c\\u8005 \\u304a \\u30b9\\u30b9\\u30e1 \\u4eac\\u90fd \\u89b3\\u5149 \\u540d\\u6240 \\u307e\\u3068\\u3081 \\u307e\\u3057 \\u305f \\u5b9a\\u756a \\u7a74\\u5834 \\u30b9\\u30dd\\u30c3\\u30c8 \\u968f\\u6642 \\u307e\\u3068\\u3081 \\u3044\\u304d \\u307e\\u3059 \\u662f\\u975e \\u4eac\\u90fd \\u89b3\\u5149 \\u53c2\\u8003 \\u3057 \\u4e0b\\u3055\\u3044',\n",
       " u'summary': u'\\u95a2\\u897f\\u306b\\u4f4f\\u3093\\u3067\\u3044\\u308b\\u4f5c\\u8005\\u304c\\u304a\\u30b9\\u30b9\\u30e1\\u306e\\u4eac\\u90fd\\u306e\\u89b3\\u5149\\u540d\\u6240\\u3092\\u307e\\u3068\\u3081\\u307e\\u3057\\u305f \\u5b9a\\u756a\\u304b\\u3089\\u7a74\\u5834\\u306e\\u30b9\\u30dd\\u30c3\\u30c8\\u3092\\u968f\\u6642\\u307e\\u3068\\u3081\\u3066\\u3044\\u304d\\u307e\\u3059\\uff01\\u662f\\u975e\\u3001\\u4eac\\u90fd\\u89b3\\u5149\\u306e\\u53c2\\u8003\\u306b\\u3057\\u3066\\u4e0b\\u3055\\u3044\\u3002',\n",
       " u'title': u'\\u3010\\u5b9a\\u756a\\u304b\\u3089\\u7a74\\u5834\\u307e\\u3067\\u3011\\u304a\\u30b9\\u30b9\\u30e1\\u306e\\u4eac\\u90fd\\u89b3\\u5149 ...- NAVER \\u307e\\u3068\\u3081',\n",
       " u'url': u'matome.naver.jp/odai/2137693040709691601'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.のコーパスはjson形式で保管されている．\n",
    "import json\n",
    "with open(\"../data/kyoto_results_100.json\", \"r\") as f:\n",
    "    docs = json.load(f)\n",
    "print(\"Num of docs = \", len(docs))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'\\u5b9a\\u756a \\u7a74\\u5834 \\u304a \\u30b9\\u30b9\\u30e1 \\u4eac\\u90fd \\u89b3\\u5149 ...- NAVER \\u307e\\u3068\\u3081 \\u95a2\\u897f \\u4f4f\\u3093 \\u3044\\u308b \\u4f5c\\u8005 \\u304a \\u30b9\\u30b9\\u30e1 \\u4eac\\u90fd \\u89b3\\u5149 \\u540d\\u6240 \\u307e\\u3068\\u3081 \\u307e\\u3057 \\u305f \\u5b9a\\u756a \\u7a74\\u5834 \\u30b9\\u30dd\\u30c3\\u30c8 \\u968f\\u6642 \\u307e\\u3068\\u3081 \\u3044\\u304d \\u307e\\u3059 \\u662f\\u975e \\u4eac\\u90fd \\u89b3\\u5149 \\u53c2\\u8003 \\u3057 \\u4e0b\\u3055\\u3044'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `bow`　には形態素解析でトークン化された単語列がスペース区切りで保存されている．\n",
    "# これを使用して特徴ベクトルを作成するとよい．\n",
    "docs[0][\"bow\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下が課題のコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "クエリ\n",
      "京都\n",
      "\n",
      "旬の京都観光情報と 京都のおすすめ観光スポット | 京都観光総 ...\n",
      "https://kyoto-design.jp\n",
      "京都観光オフィシャルサイト 京都観光Navi\n",
      "https://kanko.city.kyoto.lg.jp\n",
      "京都観光地おすすめランキング - コトログ京都\n",
      "www.kotolog.jp\n",
      "京都観光 おすすめ観光スポット・世界遺産・グルメのまとめ\n",
      "homepage2.nifty.com/otokulink/kyoto_top.html\n",
      "季節の散策コース｜観光ガイド｜そうだ 京都、行こう。～京都 ...\n",
      "souda-kyoto.jp/travel/walk\n",
      "----------------------------\n",
      "\n",
      "----------------------------\n",
      "クエリ\n",
      "観光\n",
      "\n",
      "旬の京都観光情報と 京都のおすすめ観光スポット | 京都観光総 ...\n",
      "https://kyoto-design.jp\n",
      "観光バスツアー・観光タクシー | 京都の観光&遊び・体験 ...\n",
      "www.veltra.com/jp/japan/kyoto/ctg/7185:Bus_Tours\n",
      "「 京都観光タクシー 」 格安で有意義な京都観光をお約束！\n",
      "www.kyoto.cc/index.htm\n",
      "京都観光おすすめのグルメや人気の穴場\n",
      "trendnews-kyoto-0909.com\n",
      "京都観光/旅行・ぶらり伏見\n",
      "kyoto-fushimi.sakura.ne.jp\n",
      "----------------------------\n",
      "\n",
      "----------------------------\n",
      "クエリ\n",
      "おすすめ\n",
      "\n",
      "京都観光おすすめのグルメや人気の穴場\n",
      "trendnews-kyoto-0909.com\n",
      "京都観光 | 京都の観光を楽しむならきょうと情報版\n",
      "www.kyotojoho.co.jp\n",
      "京都観光地おすすめランキング - コトログ京都\n",
      "www.kotolog.jp\n",
      "旬の京都観光情報と 京都のおすすめ観光スポット | 京都観光総 ...\n",
      "https://kyoto-design.jp\n",
      "京都観光のおすすめコースをガイド | 京都じっくり観光\n",
      "www.kyotokanko.co.jp\n",
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1.文字の読み込みとトークナイズ ---\n",
    "# スペース区切りで単語を分割する\n",
    "raw_corpus = [d[\"bow\"].split() for d in docs]\n",
    "\n",
    "# ストップワードを除去する(TODO: 日本語はどうやる?不要?)\n",
    "# corpus = corpus = [list(filter(lambda word: word not in stopwords.words(\"english\"), x)) for x in raw_corpus] \n",
    "\n",
    "# --- 2.特徴ベクトルの作成 ---\n",
    "\n",
    "# コーパスを与えて、単語->IDの辞書を作成する\n",
    "dictionary = gensim.corpora.Dictionary(raw_corpus) \n",
    "\n",
    "# 文書の単語をID化\n",
    "id_corpus = [dictionary.doc2bow(document) for document in raw_corpus]\n",
    "\n",
    "# id_courpusからIFIDFで重み付けされた特徴ベクトルを得る\n",
    "tfidf_model = gensim.models.TfidfModel(id_corpus, normalize=False) \n",
    "# id_corpusをtfidfで重み付けされたものに変換\n",
    "tfidf_corpus = tfidf_model[id_corpus] \n",
    "\n",
    "# --- 3.コサイン類似度 ---\n",
    "#今回の課題ではコサイン類似度を元にランキングを行う\n",
    "tfidf_vectors = gensim.matutils.corpus2dense(tfidf_corpus, len(dictionary)).T\n",
    "\n",
    "# クエリによる検索\n",
    "qs = [{\"京都\"}, {\"観光\"}, {\"おすすめ\"}]\n",
    "\n",
    "# クエリを特徴ベクトルに変換\n",
    "tfidf_qs = [ tfidf_model[dictionary.doc2bow(qs[i])] for i in range(3)  ]\n",
    "query_vectors =  [gensim.matutils.corpus2dense([tfidf_qs[i]], len(dictionary)).T[0]  for i in range(3)]\n",
    "\n",
    "# コサイン類似を求め、ランキングにする\n",
    "query_rankings =  [np.argsort([cosine_sim(query_vectors[i], tfidf_vectors[x]) for x in range(83)])[::-1]  for i in range(3)]\n",
    "\n",
    "# ランキングを表示する\n",
    "for i in range(3):\n",
    "    print \"----------------------------\"\n",
    "    print \"クエリ\"\n",
    "    print [\"京都\", \"観光\", \"おすすめ\"][i]\n",
    "    print \"\"\n",
    "    for j in range(5):\n",
    "        print docs[query_rankings[i][j]][\"title\"]\n",
    "        print docs[query_rankings[i][j]][\"url\"]\n",
    "    print \"----------------------------\"\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 任意課題（a） Okapi BM25\n",
    "\n",
    "上記（１）に対して， Okapi BM25 に基づくランキングを行い，上記（１）の結果と比較してみよ．また，結果が変わらない場合は，結果が変わるような文書例を自分で考えてみよ．\n",
    "\n",
    "## 任意課題（b） 適合性フィードバック\n",
    "\n",
    "適合性フィードバックによるクエリ修正を行い，検索結果がどのように変化するのか分析せよ．また，コーパス及びクエリを可視化することで，修正されたクエリが適合・不適合文書の特徴ベクトルにどのように影響されているか幾何的に分析せよ．\n",
    "\n",
    "\n",
    "# 課題の提出方法\n",
    "\n",
    "いずれかの方法で，ipython notebookのページ（.ipynbファイル）とそのhtml版を提出すること．\n",
    "\n",
    "1. 添付ファイルで山本に送信．\n",
    "   - 送付先 tyamamot at dl.kuis.kyoto-u.ac.jp\n",
    "2. 各自のgithubやgithub gistにアップロードし，そのURLを山本に送信．この場合はhtml版を用意する必要はない．\n",
    "3. 上記以外で，山本が実際に.ipynbファイルを確認できる方法．\n",
    "\n",
    "\n",
    "# 締切\n",
    "\n",
    "- 2018年1月25日（金）23:59\n",
    "- 締切に関する個別の相談は``受け付けます``．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
